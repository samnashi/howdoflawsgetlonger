{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_2lyr_mape_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_relu_ca_sigmoid_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_nlstm_tiny_relu_ca_tanh_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_ngru_micro_relu_ca_sigmoid_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_ngru_micro_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_240dense_mae_highlr_elu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_32dense_msle_rhiLR_0start_selu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_ngru_micro_relu_ca_tanh_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_240dense_msle_highlr_0start_elu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__bag_conv_lstm_nodense_micro_shufstart_relu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'loss', u'mape', u'acc', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_nlstm_tiny_relu_ca_sigmoid_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_lstm_rerun__rerun_icacsis_l1l2_.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_nlstm_micro_relu_ca_tanh_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_nlstm_micro_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('scores_conv__xgb_testmodel_smallbottleneck_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_2lyr_mse_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__mape_minmax_`100dense_relu_ca_tanh_da_3_cbd_l1_kr_HLR.csv', Index([u'Unnamed: 0', u'loss', u'mse', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['acc']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_2lyr_mae_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_240dense_mae_highlr_0start_elu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_3lyr_msle_hilr_240d_orth_init_allregzd_start0_relu_ca_sigmoid_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_2lyr_msle_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_lstm_rerun__test_safety_generator_.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_240dense_msle_highlr_0start_elu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_3lyr_msle_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('scores_conv__bag_conv_lstm_nodense_medium_shufstart_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_2lyr_mae_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('scores_conv__tree_testmodel_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_120dense_mse_highlr_0start_elu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_2lyr_mse_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('scores_conv__tree_testmodel_relu_ca_sigmoid_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_32dense_msle_rhiLR_0start_selu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('scores_conv__xgb_testmodel_smallbottleneck_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'mape', u'loss', u'acc', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_lstm_rerun__rerun_icacsis_l1l2_longer_.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_240dense_mapeelu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('scores_conv__tree_mediumbidir_nodense_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_120dense_mse_highlr_0start_elu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_lstm_rerun__rerun_icacsis_l1l2_4epochs_.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('scores_conv__xgb_testmodel_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__convnormallstm_minmax_128b_relu_ca_tanh_da_3_cbd_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_nlstm__minmax_128b_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__mape_minmax_relu_ca_tanh_da_3_cbd_l1_kr_HLR.csv', Index([u'Unnamed: 0', u'loss', u'mse', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['acc']))\n",
      "('scores_conv__tree_testmodel_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_nlstm__minmax_128b_relu_ca_tanh_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_240dense_mae_highlr_0start_elu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('scores_conv__bag_conv_lstm_nodense_micro_128d_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('scores_conv__bag_conv_lstm_nodense_medium_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_nlstm_tiny_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('scores_conv__xgb_testmodel_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_micro_240dense_msle_highlr_elu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__conv_nlstm_micro_relu_ca_sigmoid_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'acc', u'loss', u'mape', u'mae', u'filename'], dtype='object'))\n",
      "('the missing part is:', set(['mse']))\n",
      "('scores_conv__xgb_testmodel_smallbottleneck_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('scores_conv__bag_conv_lstm_nodense_medium_shufstart_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('scores_conv__tree_testmodel_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('scores_conv__tree_testmodel_relu_ca_sigmoid_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('scores_conv__xgb_testmodel_smallbottleneck_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('scores_conv__tree_mediumbidir_nodense_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('scores_conv__xgb_testmodel_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('scores_conv__tree_testmodel_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('scores_conv__bag_conv_lstm_nodense_micro_128d_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('scores_conv__bag_conv_lstm_nodense_medium_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('scores_conv__xgb_testmodel_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', 110, 7)\n",
      "('number of files', 51)\n",
      "('counter 6 cols', 40)\n",
      "('counter 110', 51)\n",
      "('list NOT ok', 11, ['scores_conv__xgb_testmodel_smallbottleneck_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv__bag_conv_lstm_nodense_medium_shufstart_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv__tree_testmodel_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv__tree_testmodel_relu_ca_sigmoid_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv__xgb_testmodel_smallbottleneck_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv__tree_mediumbidir_nodense_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv__xgb_testmodel_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv__tree_testmodel_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv__bag_conv_lstm_nodense_micro_128d_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv__bag_conv_lstm_nodense_medium_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv__xgb_testmodel_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv'])\n",
      "('list of df shapes', [(110, 6), (110, 7)])\n"
     ]
    }
   ],
   "source": [
    "from corpus_characterizer import generator_chunker\n",
    "import numpy as np\n",
    "import os\n",
    "# from sklearn.metrics import mean_squared_error,mean_absolute_error, median_absolute_error, mean_squared_log_error, explained_variance_score, r2_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, explained_variance_score, \\\n",
    "    r2_score\n",
    "import pandas as pd\n",
    "from scipy.stats import describe, kurtosistest, skewtest, normaltest\n",
    "from Conv1D_LSTM_Ensemble import pair_generator_1dconv_lstm_bagged\n",
    "from Conv1D_ActivationSearch_BigLoop import pair_generator_1dconv_lstm #NOT BAGGED\n",
    "\n",
    "# @@@@@@@@@@@@@@ RELATIVE PATHS @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "Base_Path = \"./\"\n",
    "image_path = \"./images/\"\n",
    "train_path = \"./train/\"\n",
    "test_path = \"./test/\"\n",
    "analysis_path = \"./analysis/\"\n",
    "chunker_path = analysis_path + \"chunker/\"\n",
    "preds_path = analysis_path + \"preds/\"\n",
    "results_aggregation_path = analysis_path + \"results_to_aggregate/\"\n",
    "\n",
    "#create MAIN dataframe (all models and all results) for those without MSE\n",
    "#create MAIN dataframe with MSE \n",
    "#colnames: mape, mse,mae {mean, med, stdev}, metric 1 \n",
    "\n",
    "#results to aggregate \n",
    "results_files_list = list(os.listdir(results_aggregation_path))\n",
    "counter_110 = 0\n",
    "counter_6_cols = 0\n",
    "list_all_ok = []\n",
    "list_not_ok = []\n",
    "list_of_shapes = []\n",
    "correct_colname_list = ['acc','loss','mape','mse','mae','filename']\n",
    "colname_list_no_mse = ['acc','loss','mape','mae','filename']\n",
    "#print(results_files_list)\n",
    "for results_file in results_files_list:\n",
    "    complete_filename = results_aggregation_path + results_file\n",
    "    interm_df = pd.read_csv(complete_filename)\n",
    "#     if 'filename' not in interm_df.columns:\n",
    "#         print(\"TROUBLE\", str(results_file), interm_df.columns)\n",
    "    if len(set(correct_colname_list) - set(interm_df.columns)) != 0:\n",
    "        print(\"TROUBLE, NOT ALL COLNAMES ARE IN \", str(results_file), interm_df.columns)\n",
    "        print(\"the missing part is:\", set(correct_colname_list) - set(interm_df.columns))\n",
    "    if interm_df.shape[0] >= 110:\n",
    "        counter_110 += 1\n",
    "    if interm_df.shape[1] == 6:\n",
    "        counter_6_cols += 1\n",
    "    if interm_df.shape[0] == 110 and interm_df.shape[1] == 6:\n",
    "        list_all_ok.append(results_file)\n",
    "    if interm_df.shape[0] != 110 or interm_df.shape[1] != 6:\n",
    "        print(str(results_file), interm_df.shape[0],interm_df.shape[1])\n",
    "        list_not_ok.append(results_file)\n",
    "    if interm_df.shape not in list_of_shapes:\n",
    "        list_of_shapes.append(interm_df.shape)\n",
    "    #todo: if there are 7 columns, drop the first one. read_csv usecols\n",
    "for special_file in list_not_ok:\n",
    "    complete_special_file = results_aggregation_path + str(special_file)\n",
    "    interm_df = pd.read_csv(complete_special_file)\n",
    "    if interm_df.shape[0] != 110 or interm_df.shape[1] != 6:\n",
    "        print(str(special_file), interm_df.shape[0],interm_df.shape[1])\n",
    "    if 'filename' not in interm_df.columns:\n",
    "        print(\"TROUBLE\", str(special_file), interm_df.columns)\n",
    "\n",
    "print(\"number of files\", len(results_files_list))\n",
    "print(\"counter 6 cols\", counter_6_cols)\n",
    "print(\"counter 110\", counter_110)\n",
    "print(\"list NOT ok\", len(list_not_ok), list_not_ok)\n",
    "print(\"list of df shapes\", list_of_shapes)\n",
    "# #folder\n",
    "# #list dir\n",
    "# # for each item\n",
    "# #declare new INTERMEDIATE df\n",
    "# #load each model's result into INTERMEDIATE df #check for file length.. some are 109 and some are shorter. \n",
    "# #aggregate, stdev,avg,median of INTERMEDIATE df\n",
    "\n",
    "# #save MAIN pandas df as a csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('results file: ', 'scores_conv__conv_2lyr_mape_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_relu_ca_sigmoid_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_nlstm_tiny_relu_ca_tanh_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_ngru_micro_relu_ca_sigmoid_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_ngru_micro_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_240dense_mae_highlr_elu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_32dense_msle_rhiLR_0start_selu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_ngru_micro_relu_ca_tanh_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_240dense_msle_highlr_0start_elu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__bag_conv_lstm_nodense_micro_shufstart_relu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_nlstm_tiny_relu_ca_sigmoid_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_lstm_rerun__rerun_icacsis_l1l2_.csv')\n",
      "('results file: ', 'scores_conv__conv_nlstm_micro_relu_ca_tanh_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_nlstm_micro_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__xgb_testmodel_smallbottleneck_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_2lyr_mse_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__mape_minmax_`100dense_relu_ca_tanh_da_3_cbd_l1_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_2lyr_mae_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_240dense_mae_highlr_0start_elu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_3lyr_msle_hilr_240d_orth_init_allregzd_start0_relu_ca_sigmoid_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_2lyr_msle_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_lstm_rerun__test_safety_generator_.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_240dense_msle_highlr_0start_elu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_3lyr_msle_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__bag_conv_lstm_nodense_medium_shufstart_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_2lyr_mae_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__tree_testmodel_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_120dense_mse_highlr_0start_elu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_2lyr_mse_hilr_240d_orth_init_allregzd_start0_relu_ca_tanh_da_2_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__tree_testmodel_relu_ca_sigmoid_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_32dense_msle_rhiLR_0start_selu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__xgb_testmodel_smallbottleneck_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_lstm_rerun__rerun_icacsis_l1l2_longer_.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_240dense_mapeelu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__tree_mediumbidir_nodense_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_120dense_mse_highlr_0start_elu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_lstm_rerun__rerun_icacsis_l1l2_4epochs_.csv')\n",
      "('results file: ', 'scores_conv__xgb_testmodel_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__convnormallstm_minmax_128b_relu_ca_tanh_da_3_cbd_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_nlstm__minmax_128b_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__mape_minmax_relu_ca_tanh_da_3_cbd_l1_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__tree_testmodel_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_nlstm__minmax_128b_relu_ca_tanh_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_240dense_mae_highlr_0start_elu_ca_tanh_da_3_cbd_robust_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__bag_conv_lstm_nodense_micro_128d_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__bag_conv_lstm_nodense_medium_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_nlstm_tiny_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__xgb_testmodel_relu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_micro_240dense_msle_highlr_elu_ca_tanh_da_3_cbd_minmax_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__conv_nlstm_micro_relu_ca_sigmoid_da_3_cbd_standard_sclr_l1l2_kr_HLR.csv')\n",
      "(\"mse main df's head\",                                                      mse_avg   mse_med  \\\n",
      "results_filename                                                         \n",
      "scores_conv__mape_minmax_`100dense_relu_ca_tanh...  0.999099  0.999473   \n",
      "scores_conv__mape_minmax_relu_ca_tanh_da_3_cbd_...   1.04606   1.04725   \n",
      "\n",
      "                                                     mse_stdev   mae_avg  \\\n",
      "results_filename                                                           \n",
      "scores_conv__mape_minmax_`100dense_relu_ca_tanh...  0.00162236  0.852418   \n",
      "scores_conv__mape_minmax_relu_ca_tanh_da_3_cbd_...   0.0150558  0.864678   \n",
      "\n",
      "                                                     mae_med  mae_stdev  \\\n",
      "results_filename                                                          \n",
      "scores_conv__mape_minmax_`100dense_relu_ca_tanh...  0.845221  0.0575633   \n",
      "scores_conv__mape_minmax_relu_ca_tanh_da_3_cbd_...  0.860202  0.0496441   \n",
      "\n",
      "                                                   mape_avg mape_med  \\\n",
      "results_filename                                                       \n",
      "scores_conv__mape_minmax_`100dense_relu_ca_tanh...  101.626  100.973   \n",
      "scores_conv__mape_minmax_relu_ca_tanh_da_3_cbd_...  214.308  180.921   \n",
      "\n",
      "                                                   mape_stdev mape_normd_dev  \n",
      "results_filename                                                              \n",
      "scores_conv__mape_minmax_`100dense_relu_ca_tanh...    3.24771      -0.200974  \n",
      "scores_conv__mape_minmax_relu_ca_tanh_da_3_cbd_...    166.336       -0.20072  )\n",
      "(\"no mse main df's head\",                                                      mae_avg   mae_med  \\\n",
      "results_filename                                                         \n",
      "scores_conv__conv_nlstm_micro_relu_ca_sigmoid_d...  0.886553  0.884756   \n",
      "scores_lstm_rerun__rerun_icacsis_l1l2_.csv          0.427699  0.420302   \n",
      "\n",
      "                                                    mae_stdev mape_avg  \\\n",
      "results_filename                                                         \n",
      "scores_conv__conv_nlstm_micro_relu_ca_sigmoid_d...   0.065014   252.98   \n",
      "scores_lstm_rerun__rerun_icacsis_l1l2_.csv          0.0583561  333.842   \n",
      "\n",
      "                                                   mape_med mape_stdev  \\\n",
      "results_filename                                                         \n",
      "scores_conv__conv_nlstm_micro_relu_ca_sigmoid_d...  150.434    283.102   \n",
      "scores_lstm_rerun__rerun_icacsis_l1l2_.csv          273.483     324.76   \n",
      "\n",
      "                                                   mape_normd_dev  \n",
      "results_filename                                                   \n",
      "scores_conv__conv_nlstm_micro_relu_ca_sigmoid_d...      -0.362224  \n",
      "scores_lstm_rerun__rerun_icacsis_l1l2_.csv              -0.185858  )\n"
     ]
    }
   ],
   "source": [
    "#create MAIN dataframe (all models and all results) for those without MSE\n",
    "no_mse_main_colnames =['results_filename','mae_avg','mae_med','mae_stdev','mape_avg','mape_med','mape_stdev','mape_normd_dev']\n",
    "no_mse_main_df = pd.DataFrame(columns=no_mse_main_colnames)\n",
    "no_mse_main_df.set_index(keys=['results_filename'],inplace=True)\n",
    "\n",
    "#create MAIN dataframe with MSE \n",
    "mse_main_colnames = ['results_filename','mse_avg','mse_med','mse_stdev','mae_avg','mae_med','mae_stdev','mape_avg','mape_med','mape_stdev','mape_normd_dev']\n",
    "mse_main_df = pd.DataFrame(columns=mse_main_colnames)\n",
    "mse_main_df.set_index(keys=['results_filename'], inplace=True)\n",
    "#colnames: mape, mse,mae {mean, med, stdev}, metric 1 \n",
    "\n",
    "#just do it a second time becausethe clutter is too much\n",
    "for results_file in results_files_list:\n",
    "    interm_df = pd.read_csv(results_aggregation_path + results_file)\n",
    "    print(\"results file: \", str(results_file))\n",
    "    #check if it has mse or not\n",
    "    if 'mse' not in interm_df.columns: #aggregate into the no_mse_main\n",
    "        keys_to_aggregate = ['mae','mape']\n",
    "        for key in keys_to_aggregate:\n",
    "            key_avg_colname = str(key) + \"_\" + \"avg\"\n",
    "            key_med_colname = str(key) + \"_\" + \"med\"\n",
    "            key_std_colname = str(key) + \"_\" + \"stdev\"\n",
    "#             no_mse_main_df.loc[str(results_file),[key_avg_colname]] = interm_df.loc[:,[str(key)]].mean(axis=1).values\n",
    "#             no_mse_main_df.loc[str(results_file),[key_med_colname]] = interm_df.loc[:,[str(key)]].median(axis=1).values\n",
    "#             no_mse_main_df.loc[str(results_file),[key_std_colname]] = interm_df.loc[:,[str(key)]].std(axis=1).values\n",
    "            no_mse_main_df.loc[str(results_file),[key_avg_colname]] = interm_df[str(key)].mean()\n",
    "            no_mse_main_df.loc[str(results_file),[key_med_colname]] = interm_df[str(key)].median()\n",
    "            no_mse_main_df.loc[str(results_file),[key_std_colname]] = interm_df[str(key)].std()\n",
    "            \n",
    "    if 'mse' in interm_df.columns: #aggregate into the no_mse_main\n",
    "        keys_to_aggregate = ['mse','mae','mape']\n",
    "        for key in keys_to_aggregate:\n",
    "            key_avg_colname = str(key) + \"_\" + \"avg\"\n",
    "            key_med_colname = str(key) + \"_\" + \"med\"\n",
    "            key_std_colname = str(key) + \"_\" + \"stdev\"\n",
    "#             mse_main_df.loc[str(results_file),[key_avg_colname]] = interm_df.loc[:,[str(key)]].mean(axis=1).values\n",
    "#             mse_main_df.loc[str(results_file),[key_med_colname]] = interm_df.loc[:,[str(key)]].median(axis=1).values\n",
    "#             mse_main_df.loc[str(results_file),[key_std_colname]] = interm_df.loc[:,[str(key)]].std(axis=1).values\n",
    "            mse_main_df.loc[str(results_file),[key_avg_colname]] = interm_df[str(key)].mean()\n",
    "            mse_main_df.loc[str(results_file),[key_med_colname]] = interm_df[str(key)].median()\n",
    "            mse_main_df.loc[str(results_file),[key_std_colname]] = interm_df[str(key)].std()\n",
    "\n",
    "            \n",
    "#mse_main_df.set_index(keys=['results_filename'],inplace=True) #inplace True deletes the column I used as index..\n",
    "mse_main_df.sort_values(by=['mape_avg'],inplace=True)\n",
    "mse_main_df.loc[:,['mape_normd_dev']] = (mse_main_df.loc[:,['mape_med']].values - mse_main_df.loc[:,['mape_avg']].values)/ (mse_main_df.loc[:,['mape_stdev']].values)\n",
    "print(\"mse main df's head\", mse_main_df.head(2))\n",
    "\n",
    "#no_mse_main_df.set_index(keys=['results_filename'],inplace=True)\n",
    "no_mse_main_df.sort_values(by=['mape_avg'],inplace=True)\n",
    "no_mse_main_df.loc[:,['mape_normd_dev']] = (no_mse_main_df.loc[:,['mape_med']].values - no_mse_main_df.loc[:,['mape_avg']].values)/ (no_mse_main_df.loc[:,['mape_stdev']].values)\n",
    "\n",
    "#no_mse_main_df.reset_index\n",
    "print(\"no mse main df's head\", no_mse_main_df.head(2))\n",
    "\n",
    "no_mse_main_df.to_csv(\"./analysis/no_mse_main_df.csv\")\n",
    "mse_main_df.to_csv(\"./analysis/mse_main_df.csv\")\n",
    "        #mean(axis=1)\n",
    "        #median(axis=1)\n",
    "        #std(axis=1)\n",
    "        #aggregate and stuff into the no_mse_main_df\n",
    "\n",
    "# '''This is intended to help make visualizations for what actually happens, numerically, in these sequences. It is hard to\n",
    "# describe what is going on just by plotting raw values, considering that many different scaler combinations and preprocessing\n",
    "# routines were used. \n",
    "\n",
    "# Logic flow: (use case of calculating the variation of batch statistics as time goes by)\n",
    "# load sequence (numpy.load) -> instantiate a generator chunker ->  calculate statistics on chunks yielded -> \n",
    "#  (after each chunk's characteristics have been determined) save calculation results into an array -> plot the array /\n",
    "#   describe the temporary array (== becomes the variation of batch statistics, instead of just variations of batches).'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save_arrays = True\n",
    "# CHUNKER_BATCH_SIZE = 128  # marked for calculating the endpoint for the generator loading. Sorry about the overloaded/clashing name.\n",
    "# CHUNKER_BATCH_TRAVERSAL_SIZE = 8 #how big of a batch does the chunker yield at each .next() call.\n",
    "\n",
    "# # load the preds\n",
    "# test_seqs_filenames = os.listdir(test_path + 'data/')\n",
    "# test_seqs_filenames.sort()\n",
    "\n",
    "# train_seqs_filenames = os.listdir(train_path + 'data/')\n",
    "# train_seqs_filenames.sort()\n",
    "\n",
    "# test_labels_filenames = os.listdir(test_path + 'label/')\n",
    "# test_labels_filenames.sort()\n",
    "\n",
    "# train_labels_filenames = os.listdir(train_path + 'label/')\n",
    "# train_labels_filenames.sort()\n",
    "\n",
    "# assert len(test_seqs_filenames) == len(test_labels_filenames)\n",
    "# assert len(train_seqs_filenames) == len(train_labels_filenames)\n",
    "\n",
    "# combined_test_filenames = zip(test_seqs_filenames, test_labels_filenames)\n",
    "# combined_train_filenames = zip(train_seqs_filenames, train_labels_filenames)\n",
    "\n",
    "# scalers_list = ['standard','robust','minmax','standard_per_batch','robust_per_batch','minmax_per_batch']\n",
    "# #list of scaler possibilities\n",
    "\n",
    "# batch_stats_dict = {}\n",
    "\n",
    "# #load arrays\n",
    "# #outer for: sequences\n",
    "\n",
    "# #from parser_invidivual_arrays\n",
    "# colnums_translator = ['percent_damage', 'delta_K_current_1', 'ctip_posn_curr_1',\n",
    "#                              'delta_K_current_2',\n",
    "#                              'ctip_posn_curr_2', 'delta_K_current_3', 'ctip_posn_curr_3', 'delta_K_current_4',\n",
    "#                              'ctip_posn_curr_4', 'Load_1', 'Load_2','delta_a_current_1','delta_a_current_2','delta_a_current_3','delta_a_current_4']  # and seq_id,somehow\n",
    "\n",
    "# for index_to_load in range(0, len(combined_test_filenames)):\n",
    "#     files = combined_test_filenames[index_to_load]\n",
    "#     print(\"files: {}\".format(files))\n",
    "#     test_seq_load_path = test_path + 'data/' + files[0]\n",
    "#     test_label_load_path = test_path + 'label/' + files[1]\n",
    "#     test_seq_temp = np.load(test_seq_load_path)\n",
    "#     test_labels = np.load(test_label_load_path)\n",
    "#     print(\"data/label shape: {}, {}\".format(test_seq_temp.shape,test_labels.shape))\n",
    "#     # #create a combined array where seq_temp is appended to the labels.\n",
    "#     combined_shape = (test_seq_temp.shape[0],test_seq_temp.shape[1] + test_labels.shape[1])\n",
    "#     # combined_array = np.zeros(shape=combined_shape)\n",
    "#     # combined_array[:,0:train_seq_temp.shape[1]] = train_seq_temp\n",
    "#     # combined_array[:,train_seq_temp.shape[1]:combined_array.shape[1]] = train_labels\n",
    "\n",
    "#     #print(\"train array col {}: {}\".format(col,describe(train_array[:,col],axis=0)))\n",
    "\n",
    "#     #inner for: scaler types\n",
    "#     for scaler_type_to_test in scalers_list:\n",
    "#         #part with the generator from ensemble conv-lstm\n",
    "#         #load sequence\n",
    "#         #instantiate generator, calculate for how long does the generator have to run\n",
    "\n",
    "#         gen_for_examination = \\\n",
    "#             pair_generator_1dconv_lstm_bagged(data=test_seq_temp,labels=test_labels,\n",
    "#                                               start_at = 0, use_precomputed_coeffs=False,scaler_type = scaler_type_to_test,\n",
    "#                                               generator_batch_size = 128,generator_pad = 128,label_dims=4)\n",
    "\n",
    "#         #this is the use case for computing the per-batch statistics\n",
    "#         remaining = CHUNKER_BATCH_SIZE * (test_seq_temp.shape[0] // CHUNKER_BATCH_SIZE)\n",
    "#         generator_yield_total = remaining #number of steps yielded.\n",
    "#         num_cols = test_seq_temp.shape[1] + test_labels.shape[1]#the number of columns in the features array.\n",
    "\n",
    "#         #there will be several final dataframes. Per-batch statistics, per-sequence-statistics.\n",
    "#         #1.\n",
    "#         #THIS ONE IS A TEMP!! will be analyzed.\n",
    "#         batch_stats_array = np.zeros(shape=(generator_yield_total,num_cols*4))\n",
    "#         #to store the flattened per-batch descriptions\n",
    "\n",
    "#         #have a proxy method that determines the batch size before\n",
    "\n",
    "#         temp_array_numrows = test_seq_temp.shape[0] #just the number of rows\n",
    "#         temp_array_numcols = 4 * combined_shape[1] + combined_shape[1]**2 #\n",
    "\n",
    "#         counter = 0\n",
    "#         while remaining > 0:\n",
    "#             counter = counter + 1\n",
    "#             #chunk_stats = describe chunk\n",
    "#             chunk = gen_for_examination.next()\n",
    "#             #need to reshape to a normal array. (get rid of the 1)\n",
    "#             #this is the ENSEMBLE generator's chunk.\n",
    "#             new_chunk_shape = (chunk[0][0].shape[1],chunk[0][0].shape[2] + chunk[1][0].shape[2])\n",
    "\n",
    "#             #chunk to examine SHOULD BE ALL COLUMNS!! #EXAMINE WHY LABEL HAS 5 COLUMNS\n",
    "#             data_chunk_to_examine = np.reshape(chunk[0][0],newshape=(chunk[0][0].shape[1],chunk[0][0].shape[2]))\n",
    "#             #gets rid of the additional axis on the 1st output of the generator's data output\n",
    "#             label_chunk_to_examine = np.reshape(chunk[1][0],newshape=(chunk[1][0].shape[1],chunk[1][0].shape[2]))\n",
    "\n",
    "#             #combined shape explicitly declared; same in the 0th dimension but add the 1st dimensions up.\n",
    "#             combined_chunk_to_examine_shape = (data_chunk_to_examine.shape[0],\n",
    "#                                                data_chunk_to_examine.shape[1] + label_chunk_to_examine.shape[1])\n",
    "#             chunk_to_examine = np.zeros(shape=combined_chunk_to_examine_shape)\n",
    "#             chunk_to_examine[:,0:data_chunk_to_examine.shape[1]] = data_chunk_to_examine\n",
    "#             chunk_to_examine[:,data_chunk_to_examine.shape[1]:] = label_chunk_to_examine\n",
    "\n",
    "#             #get rid of the additional axis on the 1st output of the label part of the generator's output\n",
    "#             # ; the two labels output by the generator are duplicates anyway\n",
    "#             chunk_describe_result = describe(chunk_to_examine,axis=0) #aka the lstm output.\n",
    "\n",
    "#             #chunk_stats_ndarray = dissect chunk_stats so it's saveable as a numpy array\n",
    "#             colnames_list = []\n",
    "#             chunk_mean = chunk_describe_result.mean\n",
    "#             for i in range(0, chunk_mean.shape[0]):\n",
    "#                 colnames_list.append('mean_' + colnums_translator[i])\n",
    "\n",
    "#             chunk_variance = chunk_describe_result.variance\n",
    "#             for i in range(0, chunk_mean.shape[0]):\n",
    "#                 colnames_list.append('var_' + colnums_translator[i])\n",
    "\n",
    "#             chunk_skewness = chunk_describe_result.skewness\n",
    "#             for i in range(0, chunk_mean.shape[0]):\n",
    "#                 colnames_list.append('skew_' + colnums_translator[i])\n",
    "\n",
    "#             chunk_kurtosis = chunk_describe_result.kurtosis\n",
    "#             for i in range(0, chunk_mean.shape[0]):\n",
    "#                 colnames_list.append('kur_' + colnums_translator[i])\n",
    "\n",
    "#             chunk_minmax = chunk_describe_result.minmax\n",
    "#             chunk_corrcoef = np.corrcoef(chunk_to_examine,rowvar=False)\n",
    "\n",
    "#             for rownum in range(0,chunk_corrcoef.shape[0]):\n",
    "#                 for colnum in range(0,chunk_corrcoef.shape[1]):\n",
    "#                     colnames_list.append('corrcoef_r' + colnums_translator[rownum] + \"_c\" + colnums_translator[colnum])\n",
    "#             chunk_corrcoef_flattened = chunk_corrcoef.flatten()\n",
    "#             chunk_ndarray = np.zeros(shape=(4 + chunk_to_examine.shape[1],chunk_to_examine.shape[1]))\n",
    "\n",
    "#             chunk_ndarray[0,:] = chunk_mean\n",
    "#             chunk_ndarray[1,:] = chunk_variance\n",
    "#             chunk_ndarray[2,:] = chunk_skewness\n",
    "#             chunk_ndarray[3,:] = chunk_kurtosis\n",
    "#             chunk_ndarray[4:,:] = chunk_corrcoef\n",
    "\n",
    "#             #TODO: add chunk into the batch_stats_array\n",
    "#             #flatten chunk_stats_ndarray\n",
    "#             chunk_ndarray_flattened = np.ndarray.flatten(chunk_ndarray)\n",
    "#             #print(\"chunk_ndarray_flattened shape: {}\".format(chunk_ndarray_flattened.shape))\n",
    "#             remaining = remaining - CHUNKER_BATCH_SIZE\n",
    "#             #define key (sequence name + scaler applied)\n",
    "#         key = files[0] + str(scaler_type_to_test)\n",
    "#         #dict[key] = chunk_stats_ndarray_flattened.\n",
    "#         batch_stats_dict[key] = chunk_ndarray_flattened\n",
    "#         #dict as pandas dataframe.\n",
    "\n",
    "# train_describe_result_df = pd.DataFrame.from_dict(batch_stats_dict, orient='index')\n",
    "# #todo: wrong flatten direction.\n",
    "\n",
    "# print(\"len(colnames): {} len(flattened): {} colnames_list: {}\".format(len(colnames_list),chunk_ndarray_flattened.shape,colnames_list))\n",
    "# #assert(len(colnames_list) == chunk_ndarray_flattened.shape[0])\n",
    "\n",
    "# train_describe_result_df.to_csv(analysis_path + 'test_corpus_describe_result2.csv',header=colnames_list)\n",
    "# # except:\n",
    "# #     print(\"writing the csv without headers.\")\n",
    "# #     train_describe_result_df.to_csv(analysis_path + 'train_corpus_describe_result.csv')\n",
    "# print('csv written.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO: pca whiten=True.\n",
    "##################################################### OLD STUFF BELOW\n",
    "# #load into generator\n",
    "#\n",
    "#\n",
    "# # instantiate variables\n",
    "# mse_cumulative = 0.0\n",
    "# mse_at_instance = 0.0\n",
    "# mse_average = 0.0\n",
    "# mse_at_instance_list = []\n",
    "# mse_average_list = []\n",
    "# mse_cumulative_list = []\n",
    "#\n",
    "# mae_cumulative = 0.0\n",
    "# mae_at_instance = 0.0\n",
    "# mae_average = 0.0\n",
    "# mae_at_instance_list = []\n",
    "# mae_average_list = []\n",
    "# mae_cumulative_list = []\n",
    "#\n",
    "# # med_ae_cumulative = 0.0\n",
    "# # med_ae_at_instance = 0.0\n",
    "# # med_ae_average = 0.0\n",
    "# # med_ae_at_instance_list = []\n",
    "# # med_ae_average_list = []\n",
    "# # med_ae_cumulative_list = []\n",
    "#\n",
    "# msle_cumulative = 0.0\n",
    "# msle_at_instance = 0.0\n",
    "# msle_average = 0.0\n",
    "# msle_at_instance_list = []\n",
    "# msle_average_list = []\n",
    "# msle_cumulative_list = []\n",
    "#\n",
    "# evs_cumulative = 0.0\n",
    "# evs_at_instance = 0.0\n",
    "# evs_average = 0.0\n",
    "# evs_at_instance_list = []\n",
    "# evs_average_list = []\n",
    "# evs_cumulative_list = []\n",
    "#\n",
    "# r2_cumulative = 0.0\n",
    "# r2_at_instance = 0.0\n",
    "# r2_average = 0.0\n",
    "# r2_at_instance_list = []\n",
    "# r2_average_list = []\n",
    "# r2_cumulative_list = []\n",
    "#\n",
    "# # for index_to_load in range(0,2):\n",
    "# for index_to_load in range(0, len(test_seqs_filenames)):\n",
    "#\n",
    "#     mse_cumulative = 0.0\n",
    "#     mse_at_instance = 0.0\n",
    "#     mse_average = 0.0\n",
    "#     mse_at_instance_list = []\n",
    "#     mse_average_list = []\n",
    "#     mse_cumulative_list = []\n",
    "#\n",
    "#     mae_cumulative = 0.0\n",
    "#     mae_at_instance = 0.0\n",
    "#     mae_average = 0.0\n",
    "#     mae_at_instance_list = []\n",
    "#     mae_average_list = []\n",
    "#     mae_cumulative_list = []\n",
    "#\n",
    "#     # med_ae_cumulative = 0.0\n",
    "#     # med_ae_at_instance = 0.0\n",
    "#     # med_ae_average = 0.0\n",
    "#     # med_ae_at_instance_list = []\n",
    "#     # med_ae_average_list = []\n",
    "#     # med_ae_cumulative_list = []\n",
    "#\n",
    "#     # msle_cumulative = 0.0\n",
    "#     # msle_at_instance = 0.0\n",
    "#     # msle_average = 0.0\n",
    "#     # msle_at_instance_list = []\n",
    "#     # msle_average_list = []\n",
    "#     # msle_cumulative_list = []\n",
    "#\n",
    "#     evs_cumulative = 0.0\n",
    "#     evs_at_instance = 0.0\n",
    "#     evs_average = 0.0\n",
    "#     evs_at_instance_list = []\n",
    "#     evs_average_list = []\n",
    "#     evs_cumulative_list = []\n",
    "#\n",
    "#     r2_cumulative = 0.0\n",
    "#     r2_at_instance = 0.0\n",
    "#     r2_average = 0.0\n",
    "#     r2_at_instance_list = []\n",
    "#     r2_average_list = []\n",
    "#     r2_cumulative_list = []\n",
    "#\n",
    "#     files = combined_filenames[index_to_load]\n",
    "#     print(\"files: {}\".format(files))\n",
    "#     preds_load_path = preds_path + files[0]\n",
    "#     test_label_load_path = test_labels_path + files[1]\n",
    "#     preds_array_temp = np.load(preds_load_path)\n",
    "#     label_test_array = np.load(test_label_load_path)\n",
    "#     print(\"before changing. preds shape: {}, label shape: {}\".format(preds_array_temp.shape, label_test_array.shape))\n",
    "#     if label_test_array.shape[1] > 5:\n",
    "#         label_test_array = label_test_array[:, 1:]\n",
    "#\n",
    "#     # TODO: reshape the preds.\n",
    "#     preds_array = np.reshape(preds_array_temp, newshape=(preds_array_temp.shape[1], 4))\n",
    "#     identifier = files[1][:-4]\n",
    "#     mse_full = mean_squared_error(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0], 1:])\n",
    "#     # mse_full_vw = mean_squared_error(y_pred=preds_array,y_true=label_test_array[0:preds_array.shape[0],1:],multioutput='variance_weighted')\n",
    "#     mae_full = mean_absolute_error(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0], 1:])\n",
    "#     # mae_full_vw = mean_absolute_error(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0],1:],multioutput='variance_weighted')\n",
    "#     r2_full = r2_score(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0], 1:])\n",
    "#     # r2_full_vw = r2_score(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0],1:],multioutput='variance_weighted')\n",
    "#     evs_full = explained_variance_score(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0], 1:])\n",
    "#     # evs_full_vw = explained_variance_score(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0],1:],multioutput='variance_weighted')\n",
    "#\n",
    "#     # if train_array.shape[1] > 11:\n",
    "#     #     train_array = train_array[:,1:]\n",
    "#\n",
    "#     # identifier = files[0][:-4]\n",
    "#\n",
    "#     # TODO load predictions\n",
    "#     # TODO load labels\n",
    "#     # initialize two sklearn metrics\n",
    "#\n",
    "#     # loss_cumulative = loss_temp + mean_squared_error(y_true=label_train_array,y_pred=train_array[:,-4:])\n",
    "#     # loss_at_instance = mean_squared_error(y_true=None,y_pred=None)\n",
    "#     # loss_average = loss_cumulative / 5#counter #BASED ON LOSS CUMULATIVE\n",
    "#     # loss_instance_avg = ?\n",
    "#\n",
    "#     chunker_proto_preds = generator_chunker(array_raw=preds_array, chunker_batch_size=CHUNKER_BATCH_TRAVERSAL_SIZE,\n",
    "#                                             start_at=0,\n",
    "#                                             scaler_active=False)\n",
    "#     chunker_proto_label = generator_chunker(array_raw=label_test_array, chunker_batch_size=CHUNKER_BATCH_TRAVERSAL_SIZE,\n",
    "#                                             start_at=0,\n",
    "#                                             scaler_active=True, scaler_type='standard_per_batch')\n",
    "#\n",
    "#     remaining = CHUNKER_BATCH_SIZE * (preds_array.shape[0] // CHUNKER_BATCH_SIZE)\n",
    "#     counter = 0\n",
    "#     # TODO modify this. load both generators and just accumulate the loss.\n",
    "#     while remaining > 0:\n",
    "#         counter = counter + 1\n",
    "#         chunk_preds = chunker_proto_preds.next()\n",
    "#         # chunk_data = chunk_data[:,-4:] #dummy, just cut the array to the last 4 columns.\n",
    "#         chunk_label = chunker_proto_label.next()\n",
    "#         chunk_label = chunk_label[:, 1:]\n",
    "#\n",
    "#         # MSE\n",
    "#         mse_at_instance = mean_squared_error(y_true=chunk_label, y_pred=chunk_preds)\n",
    "#         mse_at_instance_list.append(mse_at_instance)\n",
    "#         mse_cumulative = mse_cumulative + mse_at_instance\n",
    "#         mse_cumulative_list.append(mse_cumulative)\n",
    "#         mse_average = mse_cumulative / counter\n",
    "#         mse_average_list.append(mse_average)\n",
    "#\n",
    "#         # MAE\n",
    "#         mae_at_instance = mean_absolute_error(y_true=chunk_label, y_pred=chunk_preds)\n",
    "#         mae_at_instance_list.append(mae_at_instance)\n",
    "#         mae_cumulative = mae_cumulative + mae_at_instance\n",
    "#         mae_cumulative_list.append(mae_cumulative)\n",
    "#         mae_average = mae_cumulative / counter\n",
    "#         mae_average_list.append(mae_average)\n",
    "#\n",
    "#         # MSLE\n",
    "#         # msle_at_instance = mean_squared_log_error(y_true=chunk_label,y_pred=chunk_data)\n",
    "#         # msle_at_instance_list.append(msle_at_instance)\n",
    "#         # msle_cumulative = msle_cumulative + msle_at_instance\n",
    "#         # msle_cumulative_list.append(msle_cumulative)\n",
    "#         # msle_average = msle_cumulative/counter\n",
    "#         # msle_average_list.append(msle_average)\n",
    "#\n",
    "#         # R2\n",
    "#         r2_at_instance = r2_score(y_true=chunk_label, y_pred=chunk_preds)\n",
    "#         r2_at_instance_list.append(r2_at_instance)\n",
    "#         r2_cumulative = r2_cumulative + r2_at_instance\n",
    "#         r2_cumulative_list.append(r2_cumulative)\n",
    "#         r2_average = r2_cumulative / counter\n",
    "#         r2_average_list.append(r2_average)\n",
    "#\n",
    "#         # EVS\n",
    "#         evs_at_instance = explained_variance_score(y_true=chunk_label, y_pred=chunk_preds)\n",
    "#         evs_at_instance_list.append(evs_at_instance)\n",
    "#         evs_cumulative = evs_cumulative + evs_at_instance\n",
    "#         evs_cumulative_list.append(evs_cumulative)\n",
    "#         evs_average = evs_cumulative / counter\n",
    "#         evs_average_list.append(evs_average)\n",
    "#\n",
    "#         # Med_AE can't do multiple columns at once!\n",
    "#         # med_ae_at_instance = median_absolute_error(y_true=chunk_label,y_pred=chunk_data) #\n",
    "#         # med_ae_at_instance_list.append(med_ae_at_instance)\n",
    "#         # med_ae_cumulative = med_ae_cumulative + med_ae_at_instance\n",
    "#         # med_ae_cumulative_list.append(med_ae_cumulative)\n",
    "#         # med_ae_average = med_ae_cumulative/counter\n",
    "#         # med_ae_average_list.append(med_ae_average)\n",
    "#\n",
    "#         print(\"remaining: {}\".format(remaining))\n",
    "#         remaining = remaining - CHUNKER_BATCH_SIZE\n",
    "#         # print(\"data chunk 2: {}\".format(chunker_proto_data.next()))\n",
    "#\n",
    "#     aggregate_list = []  # saves the aggregated array in a list so the filename saving can be automated\n",
    "#     aggregate_name_list = []  # since trying to directly access variable names isn't a good idea in python...\n",
    "#     # MSE MAE MSLE R2 EVS MED_AE\n",
    "#     print(\"mse_cumulative: {}\".format(mse_cumulative_list))\n",
    "#     print(\"mse_average: {}\".format(mse_average_list))\n",
    "#     print(\"mse_at_instance: {}\".format(mse_at_instance_list))\n",
    "#     assert (len(mse_cumulative_list) == len(mse_average_list) == len(mse_at_instance_list))\n",
    "#     aggregate_mse = np.empty(shape=(len(mse_cumulative_list), 4))\n",
    "#     # ORDER IS cumulative - average - at instance\n",
    "#     aggregate_mse[:, 0] = np.asarray(mse_cumulative_list)\n",
    "#     aggregate_mse[:, 1] = np.asarray(mse_average_list)\n",
    "#     aggregate_mse[:, 2] = np.asarray(mse_at_instance_list)\n",
    "#     aggregate_mse[0, 3] = mse_full\n",
    "#     # aggregate_mse[1, 3] = mse_full_vw\n",
    "#     aggregate_list.append(aggregate_mse)\n",
    "#     aggregate_name_list.append('mse_agg')\n",
    "#\n",
    "#     print(\"mae_cumulative: {}\".format(mae_cumulative_list))\n",
    "#     print(\"mae_average: {}\".format(mae_average_list))\n",
    "#     print(\"mae_at_instance: {}\".format(mae_at_instance_list))\n",
    "#     assert (len(mae_cumulative_list) == len(mae_average_list) == len(mae_at_instance_list))\n",
    "#     aggregate_mae = np.empty(shape=(len(mae_cumulative_list), 4))\n",
    "#     # ORDER IS cumulative - average - at instance\n",
    "#     aggregate_mae[:, 0] = np.asarray(mae_cumulative_list)\n",
    "#     aggregate_mae[:, 1] = np.asarray(mae_average_list)\n",
    "#     aggregate_mae[:, 2] = np.asarray(mae_at_instance_list)\n",
    "#     aggregate_mae[0, 3] = mae_full\n",
    "#     aggregate_mse[1:, 3] = 0.0\n",
    "#     aggregate_list.append(aggregate_mae)\n",
    "#     aggregate_name_list.append('mae_agg')\n",
    "#\n",
    "#     # print(\"msle_cumulative: {}\".format(msle_cumulative_list))\n",
    "#     # print(\"msle_average: {}\".format(msle_average_list))\n",
    "#     # print(\"msle_at_instance: {}\".format(msle_at_instance_list))\n",
    "#     # assert(len(msle_cumulative_list)==len(msle_average_list)==len(msle_at_instance_list))\n",
    "#     # aggregate_msle = np.empty(shape=(len(msle_cumulative_list),4))\n",
    "#     # #ORDER IS cumulative - average - at instance\n",
    "#     # aggregate_msle[:, 0] = np.asarray(msle_cumulative_list)\n",
    "#     # aggregate_msle[:, 1] = np.asarray(msle_average_list)\n",
    "#     # aggregate_msle[:, 2] = np.asarray(msle_at_instance_list)\n",
    "#     # aggregate_msle[0, 3] = msle_full\n",
    "#     # aggregate_list.append(aggregate_msle)\n",
    "#     # aggregate_name_list.append(\"msle\")\n",
    "#\n",
    "#     print(\"r2_cumulative: {}\".format(r2_cumulative_list))\n",
    "#     print(\"r2_average: {}\".format(r2_average_list))\n",
    "#     print(\"r2_at_instance: {}\".format(r2_at_instance_list))\n",
    "#     assert (len(r2_cumulative_list) == len(r2_average_list) == len(r2_at_instance_list))\n",
    "#     aggregate_r2 = np.empty(shape=(len(r2_cumulative_list), 4))\n",
    "#     # ORDER IS cumulative - average - at instance\n",
    "#     aggregate_r2[:, 0] = np.asarray(r2_cumulative_list)\n",
    "#     aggregate_r2[:, 1] = np.asarray(r2_average_list)\n",
    "#     aggregate_r2[:, 2] = np.asarray(r2_at_instance_list)\n",
    "#     aggregate_r2[0, 3] = r2_full\n",
    "#     aggregate_list.append(aggregate_r2)\n",
    "#     aggregate_name_list.append('r2_agg')\n",
    "#\n",
    "#     print(\"evs_cumulative: {}\".format(evs_cumulative_list))\n",
    "#     print(\"evs_average: {}\".format(evs_average_list))\n",
    "#     print(\"evs_at_instance: {}\".format(evs_at_instance_list))\n",
    "#     assert (len(evs_cumulative_list) == len(evs_average_list) == len(evs_at_instance_list))\n",
    "#     aggregate_evs = np.empty(shape=(len(evs_cumulative_list), 4))\n",
    "#     # ORDER IS cumulative - average - at instance\n",
    "#     aggregate_evs[:, 0] = np.asarray(evs_cumulative_list)  # cumulative.\n",
    "#     aggregate_evs[:, 1] = np.asarray(evs_average_list)\n",
    "#     aggregate_evs[:, 2] = np.asarray(evs_at_instance_list)\n",
    "#     aggregate_evs[0, 3] = evs_full\n",
    "#     aggregate_list.append(aggregate_evs)\n",
    "#     aggregate_name_list.append('evs_agg')\n",
    "#\n",
    "#     if save_arrays == True:\n",
    "#         for index in range(0, len(aggregate_list)):\n",
    "#             # get the index of the array in the list of names (the second list)\n",
    "#             arrayname = aggregate_name_list[index]\n",
    "#             np.savetxt(fname=chunker_path + arrayname + \"_\" + str(identifier) + \".csv\", delimiter=',',\n",
    "#                        X=aggregate_list[index], header=\"cumulative(sum)-average-instance\", fmt='%.18e')\n",
    "#\n",
    "#\n",
    "#             # TODO: aggregate and save as a numpy array or a csv.\n",
    "#\n",
    "#             # print(\"med_ae_cumulative: {}\".format(med_ae_cumulative_list))\n",
    "#             # print(\"med_ae_average: {}\".format(med_ae_average_list))\n",
    "#             # print(\"med_ae_at_instance: {}\".format(med_ae_at_instance_list))\n",
    "#             # print(\"label chunk 1: {}\".format(chunker_proto_label.next()))\n",
    "#             # print(\"label chunk 2: {}\".format(chunker_proto_label.next()))\n",
    "#\n",
    "#             # if (str(files[0]) == 'sequence_2c_288_9_fv1b.npy') == True:\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:, 0], '^', label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:, 0], '.', label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.75 * (len(y_prediction)), 1 * (len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction from 75% - 100% of the sequence on Crack 01')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_0_conv_75_100_newmarker_batch' + str(\n",
    "#             #         generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:, 1], '^', label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:, 1], 'v', label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.75 * (len(y_prediction)), 1 * (len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 75% - 100% of the sequence on Crack 02')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_1_conv_75_100_newmarker_batch' + str(\n",
    "#             #         generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:, 2], '^', label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:, 2], 'v', label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.75 * (len(y_prediction)), 1 * (len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 75% - 100% of the sequence on Crack 03')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_2_conv_75_100_newmarker_batch' + str(\n",
    "#             #         generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:, 3], '^', label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:, 3], 'v', label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.75 * (len(y_prediction)), 1 * (len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 75% - 100% of the sequence on Crack 04')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_3_conv_75_100_newmarker_batch' + str(\n",
    "#             #         generator_batch_size) + '_.png')\n",
    "#             # DEVIN PLOT CODE\n",
    "#             # if save_figs == True:\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:,0],'^',label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:,0],'.',label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.5*(len(y_prediction)), 1*(len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction from 50% - 100% of the sequence on Crack 01')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_0_conv_50_100_newmarker_batch' + str(generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:,1],'^',label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:,1],'v',label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.5*(len(y_prediction)), 1*(len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 50% - 100% of the sequence on Crack 02')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_1_conv_50_100_newmarker_batch' + str(generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:,2],'^',label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:,2],'v',label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.5*(len(y_prediction)), 1*(len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 50% - 100% of the sequence on Crack 03')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_2_conv_50_100_newmarker_batch' + str(generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:,3],'^',label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:,3],'v',label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.5*(len(y_prediction)), 1*(len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 50% - 100% of the sequence on Crack 04')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_3_conv_50_100_newmarker_batch' + str(generator_batch_size) + '_.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
