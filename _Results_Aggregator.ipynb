{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_rf5_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_rf5_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_linreg_model__tiny_bidir_1sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_linreg_model__tiny_bidir_1sd.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_et30_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_et30_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__tree_medbidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'loss', u'lstm_output_mean_absolute_error', u'mae',\n",
      "       u'lstm_output_loss', u'mse', u'seq_name',\n",
      "       u'lstm_output_mean_squared_logarithmic_error',\n",
      "       u'lstm_output_mean_absolute_percentage_error',\n",
      "       u'lstm_output_mean_squared_error', u'msle', u'mape',\n",
      "       u'combined_output_loss'],\n",
      "      dtype='object'))\n",
      "('the missing part is:', set(['acc', 'filename']))\n",
      "('scores_conv__tree_medbidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 13)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_lstm_rerun__bidir100ea_fv1c_.csv', Index([u'Unnamed: 0', u'loss', u'msle', u'mae', u'mape', u'seq_name', u'mse'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'filename']))\n",
      "('scores_lstm_rerun__bidir100ea_fv1c_.csv', 108, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_sk_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'seq_name', u'mse', u'mse_f3', u'mae', u'mae_f3'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('scores_sk_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 6)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_fv1cridgecholesky_50sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_fv1cridgecholesky_50sd.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_elasticnet__bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_elasticnet__bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_fv1cridgesaga_50sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_fv1cridgesaga_50sd.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_fv1c_elasticnet_50sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_fv1c_elasticnet_50sd.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_lstm_rerun__tiny_bidir_.csv', Index([u'Unnamed: 0', u'loss', u'msle', u'mae', u'mape', u'seq_name', u'mse'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'filename']))\n",
      "('scores_lstm_rerun__tiny_bidir_.csv', 108, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv__tree_tinybidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'loss', u'lstm_output_mean_absolute_error', u'mae',\n",
      "       u'lstm_output_loss', u'mse', u'seq_name',\n",
      "       u'lstm_output_mean_squared_logarithmic_error',\n",
      "       u'lstm_output_mean_absolute_percentage_error',\n",
      "       u'lstm_output_mean_squared_error',\n",
      "       u'combined_output_mean_squared_logarithmic_error', u'mape',\n",
      "       u'combined_output_loss'],\n",
      "      dtype='object'))\n",
      "('the missing part is:', set(['acc', 'filename']))\n",
      "('scores_conv__tree_tinybidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 13)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'loss', u'lstm_output_mean_absolute_error',\n",
      "       u'lstm_output_acc', u'mae', u'lstm_output_loss', u'mse',\n",
      "       u'lstm_output_mean_squared_logarithmic_error',\n",
      "       u'lstm_output_mean_squared_error',\n",
      "       u'lstm_output_mean_absolute_percentage_error', u'seq_name',\n",
      "       u'combined_output_acc', u'mape', u'msle', u'combined_output_loss'],\n",
      "      dtype='object'))\n",
      "('the missing part is:', set(['acc', 'filename']))\n",
      "('scores_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 15)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_lstm_rerun__big_bidir_.csv', Index([u'Unnamed: 0', u'loss', u'msle', u'mae', u'mape', u'seq_name', u'mse'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'filename']))\n",
      "('scores_lstm_rerun__big_bidir_.csv', 108, 7)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_fv1clinreg_50sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_fv1clinreg_50sd.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_rf30_ws__model__tiny_bidir_.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_rf30_ws__model__tiny_bidir_.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_elasticnet_model__tiny_bidir_20sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_elasticnet_model__tiny_bidir_20sd.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_fv1crf60_ws__.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_fv1crf60_ws__.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_ridgesaga_model__tiny_bidir_20sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_ridgesaga_model__tiny_bidir_20sd.csv', 108, 4)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'scores_sk_conv_lstm_bagged__bag_conv_lstm_nodense_micro_128d_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'seq_name', u'mse', u'mse_f3', u'mae', u'mae_f3'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('scores_sk_conv_lstm_bagged__bag_conv_lstm_nodense_micro_128d_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 6)\n",
      "('TROUBLE, NOT ALL COLNAMES ARE IN ', 'combi_scores_et5_ws__model__tiny_bidir_20sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('the missing part is:', set(['acc', 'loss', 'filename', 'mape']))\n",
      "('combi_scores_et5_ws__model__tiny_bidir_20sd.csv', 108, 4)\n",
      "('combi_scores_rf5_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_rf5_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('combi_scores_linreg_model__tiny_bidir_1sd.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_linreg_model__tiny_bidir_1sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('combi_scores_et30_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_et30_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('scores_conv__tree_medbidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 13)\n",
      "('TROUBLE', 'scores_conv__tree_medbidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'loss', u'lstm_output_mean_absolute_error', u'mae',\n",
      "       u'lstm_output_loss', u'mse', u'seq_name',\n",
      "       u'lstm_output_mean_squared_logarithmic_error',\n",
      "       u'lstm_output_mean_absolute_percentage_error',\n",
      "       u'lstm_output_mean_squared_error', u'msle', u'mape',\n",
      "       u'combined_output_loss'],\n",
      "      dtype='object'))\n",
      "('scores_lstm_rerun__bidir100ea_fv1c_.csv', 108, 7)\n",
      "('TROUBLE', 'scores_lstm_rerun__bidir100ea_fv1c_.csv', Index([u'Unnamed: 0', u'loss', u'msle', u'mae', u'mape', u'seq_name', u'mse'], dtype='object'))\n",
      "('scores_sk_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 6)\n",
      "('TROUBLE', 'scores_sk_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'seq_name', u'mse', u'mse_f3', u'mae', u'mae_f3'], dtype='object'))\n",
      "('combi_scores_fv1cridgecholesky_50sd.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_fv1cridgecholesky_50sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('combi_scores_elasticnet__bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_elasticnet__bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('combi_scores_fv1cridgesaga_50sd.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_fv1cridgesaga_50sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('combi_scores_fv1c_elasticnet_50sd.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_fv1c_elasticnet_50sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('scores_lstm_rerun__tiny_bidir_.csv', 108, 7)\n",
      "('TROUBLE', 'scores_lstm_rerun__tiny_bidir_.csv', Index([u'Unnamed: 0', u'loss', u'msle', u'mae', u'mape', u'seq_name', u'mse'], dtype='object'))\n",
      "('scores_conv__tree_tinybidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 13)\n",
      "('TROUBLE', 'scores_conv__tree_tinybidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'loss', u'lstm_output_mean_absolute_error', u'mae',\n",
      "       u'lstm_output_loss', u'mse', u'seq_name',\n",
      "       u'lstm_output_mean_squared_logarithmic_error',\n",
      "       u'lstm_output_mean_absolute_percentage_error',\n",
      "       u'lstm_output_mean_squared_error',\n",
      "       u'combined_output_mean_squared_logarithmic_error', u'mape',\n",
      "       u'combined_output_loss'],\n",
      "      dtype='object'))\n",
      "('scores_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 15)\n",
      "('TROUBLE', 'scores_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'loss', u'lstm_output_mean_absolute_error',\n",
      "       u'lstm_output_acc', u'mae', u'lstm_output_loss', u'mse',\n",
      "       u'lstm_output_mean_squared_logarithmic_error',\n",
      "       u'lstm_output_mean_squared_error',\n",
      "       u'lstm_output_mean_absolute_percentage_error', u'seq_name',\n",
      "       u'combined_output_acc', u'mape', u'msle', u'combined_output_loss'],\n",
      "      dtype='object'))\n",
      "('scores_lstm_rerun__big_bidir_.csv', 108, 7)\n",
      "('TROUBLE', 'scores_lstm_rerun__big_bidir_.csv', Index([u'Unnamed: 0', u'loss', u'msle', u'mae', u'mape', u'seq_name', u'mse'], dtype='object'))\n",
      "('combi_scores_fv1clinreg_50sd.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_fv1clinreg_50sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('combi_scores_rf30_ws__model__tiny_bidir_.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_rf30_ws__model__tiny_bidir_.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('combi_scores_elasticnet_model__tiny_bidir_20sd.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_elasticnet_model__tiny_bidir_20sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('combi_scores_fv1crf60_ws__.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_fv1crf60_ws__.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('combi_scores_ridgesaga_model__tiny_bidir_20sd.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_ridgesaga_model__tiny_bidir_20sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('scores_sk_conv_lstm_bagged__bag_conv_lstm_nodense_micro_128d_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 108, 6)\n",
      "('TROUBLE', 'scores_sk_conv_lstm_bagged__bag_conv_lstm_nodense_micro_128d_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', Index([u'Unnamed: 0', u'seq_name', u'mse', u'mse_f3', u'mae', u'mae_f3'], dtype='object'))\n",
      "('combi_scores_et5_ws__model__tiny_bidir_20sd.csv', 108, 4)\n",
      "('TROUBLE', 'combi_scores_et5_ws__model__tiny_bidir_20sd.csv', Index([u'seq_name', u'r2', u'mse', u'mae'], dtype='object'))\n",
      "('number of files', 21)\n",
      "('counter 6 cols', 2)\n",
      "('counter 110', 0)\n",
      "('list NOT ok', 21, ['combi_scores_rf5_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'combi_scores_linreg_model__tiny_bidir_1sd.csv', 'combi_scores_et30_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv__tree_medbidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_lstm_rerun__bidir100ea_fv1c_.csv', 'scores_sk_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'combi_scores_fv1cridgecholesky_50sd.csv', 'combi_scores_elasticnet__bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'combi_scores_fv1cridgesaga_50sd.csv', 'combi_scores_fv1c_elasticnet_50sd.csv', 'scores_lstm_rerun__tiny_bidir_.csv', 'scores_conv__tree_tinybidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'scores_lstm_rerun__big_bidir_.csv', 'combi_scores_fv1clinreg_50sd.csv', 'combi_scores_rf30_ws__model__tiny_bidir_.csv', 'combi_scores_elasticnet_model__tiny_bidir_20sd.csv', 'combi_scores_fv1crf60_ws__.csv', 'combi_scores_ridgesaga_model__tiny_bidir_20sd.csv', 'scores_sk_conv_lstm_bagged__bag_conv_lstm_nodense_micro_128d_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv', 'combi_scores_et5_ws__model__tiny_bidir_20sd.csv'])\n",
      "('list of df shapes', [(108, 4), (108, 13), (108, 7), (108, 6), (108, 15)])\n"
     ]
    }
   ],
   "source": [
    "from corpus_characterizer import generator_chunker\n",
    "import numpy as np\n",
    "import os\n",
    "# from sklearn.metrics import mean_squared_error,mean_absolute_error, median_absolute_error, mean_squared_log_error, explained_variance_score, r2_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, explained_variance_score, \\\n",
    "    r2_score\n",
    "import pandas as pd\n",
    "from scipy.stats import describe, kurtosistest, skewtest, normaltest\n",
    "from Conv1D_LSTM_Ensemble import pair_generator_1dconv_lstm_bagged\n",
    "from Conv1D_ActivationSearch_BigLoop import pair_generator_1dconv_lstm #NOT BAGGED\n",
    "\n",
    "# @@@@@@@@@@@@@@ RELATIVE PATHS @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "Base_Path = \"./\"\n",
    "image_path = \"./images/\"\n",
    "train_path = \"./train/\"\n",
    "test_path = \"./test/\"\n",
    "analysis_path = \"./analysis/\"\n",
    "chunker_path = analysis_path + \"chunker/\"\n",
    "preds_path = analysis_path + \"preds/\"\n",
    "results_aggregation_path = analysis_path + \"results_to_aggregate/\"\n",
    "\n",
    "#create MAIN dataframe (all models and all results) for those without MSE\n",
    "#create MAIN dataframe with MSE \n",
    "#colnames: mape, mse,mae {mean, med, stdev}, metric 1 \n",
    "\n",
    "#results to aggregate \n",
    "results_files_list = list(os.listdir(results_aggregation_path))\n",
    "counter_110 = 0\n",
    "counter_6_cols = 0\n",
    "list_all_ok = []\n",
    "list_not_ok = []\n",
    "list_of_shapes = []\n",
    "correct_colname_list = ['acc','loss','mape','mse','mae','filename']\n",
    "colname_list_no_mse = ['acc','loss','mape','mae','filename']\n",
    "#print(results_files_list)\n",
    "for results_file in results_files_list:\n",
    "    complete_filename = results_aggregation_path + results_file\n",
    "    interm_df = pd.read_csv(complete_filename)\n",
    "#     if 'filename' not in interm_df.columns:\n",
    "#         print(\"TROUBLE\", str(results_file), interm_df.columns)\n",
    "    if len(set(correct_colname_list) - set(interm_df.columns)) != 0:\n",
    "        print(\"TROUBLE, NOT ALL COLNAMES ARE IN \", str(results_file), interm_df.columns)\n",
    "        print(\"the missing part is:\", set(correct_colname_list) - set(interm_df.columns))\n",
    "    if interm_df.shape[0] >= 110:\n",
    "        counter_110 += 1\n",
    "    if interm_df.shape[1] == 6:\n",
    "        counter_6_cols += 1\n",
    "    if interm_df.shape[0] == 110 and interm_df.shape[1] == 6:\n",
    "        list_all_ok.append(results_file)\n",
    "    if interm_df.shape[0] != 110 or interm_df.shape[1] != 6:\n",
    "        print(str(results_file), interm_df.shape[0],interm_df.shape[1])\n",
    "        list_not_ok.append(results_file)\n",
    "    if interm_df.shape not in list_of_shapes:\n",
    "        list_of_shapes.append(interm_df.shape)\n",
    "    #todo: if there are 7 columns, drop the first one. read_csv usecols\n",
    "for special_file in list_not_ok:\n",
    "    complete_special_file = results_aggregation_path + str(special_file)\n",
    "    interm_df = pd.read_csv(complete_special_file)\n",
    "    if interm_df.shape[0] != 110 or interm_df.shape[1] != 6:\n",
    "        print(str(special_file), interm_df.shape[0],interm_df.shape[1])\n",
    "    if 'filename' not in interm_df.columns:\n",
    "        print(\"TROUBLE\", str(special_file), interm_df.columns)\n",
    "\n",
    "print(\"number of files\", len(results_files_list))\n",
    "print(\"counter 6 cols\", counter_6_cols)\n",
    "print(\"counter 110\", counter_110)\n",
    "print(\"list NOT ok\", len(list_not_ok), list_not_ok)\n",
    "print(\"list of df shapes\", list_of_shapes)\n",
    "# #folder\n",
    "# #list dir\n",
    "# # for each item\n",
    "# #declare new INTERMEDIATE df\n",
    "# #load each model's result into INTERMEDIATE df #check for file length.. some are 109 and some are shorter. \n",
    "# #aggregate, stdev,avg,median of INTERMEDIATE df\n",
    "\n",
    "# #save MAIN pandas df as a csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('results file: ', 'combi_scores_rf5_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'combi_scores_linreg_model__tiny_bidir_1sd.csv')\n",
      "('results file: ', 'combi_scores_et30_ws___bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv__tree_medbidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_lstm_rerun__bidir100ea_fv1c_.csv')\n",
      "('results file: ', 'scores_sk_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'combi_scores_fv1cridgecholesky_50sd.csv')\n",
      "('results file: ', 'combi_scores_elasticnet__bag_convdblfilters_lstm_nodense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'combi_scores_fv1cridgesaga_50sd.csv')\n",
      "('results file: ', 'combi_scores_fv1c_elasticnet_50sd.csv')\n",
      "('results file: ', 'scores_lstm_rerun__tiny_bidir_.csv')\n",
      "('results file: ', 'scores_conv__tree_tinybidir_nodense_fv1c_slowlr_relu_ca_sigmoid_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_conv_lstm_bagged__bag_conv_lstm_dense_tiny_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'scores_lstm_rerun__big_bidir_.csv')\n",
      "('results file: ', 'combi_scores_fv1clinreg_50sd.csv')\n",
      "('results file: ', 'combi_scores_rf30_ws__model__tiny_bidir_.csv')\n",
      "('results file: ', 'combi_scores_elasticnet_model__tiny_bidir_20sd.csv')\n",
      "('results file: ', 'combi_scores_fv1crf60_ws__.csv')\n",
      "('results file: ', 'combi_scores_ridgesaga_model__tiny_bidir_20sd.csv')\n",
      "('results file: ', 'scores_sk_conv_lstm_bagged__bag_conv_lstm_nodense_micro_128d_shufstart_relu_ca_tanh_da_3_cbd_standard_per_batch_sclr_l1l2_kr_HLR.csv')\n",
      "('results file: ', 'combi_scores_et5_ws__model__tiny_bidir_20sd.csv')\n",
      "(\"mse main df's head\",                                                        mse_avg     mse_med  \\\n",
      "results_filename                                                             \n",
      "combi_scores_elasticnet__bag_convdblfilters_lst...  0.00904585  0.00908431   \n",
      "combi_scores_elasticnet_model__tiny_bidir_20sd.csv  0.00904648  0.00908492   \n",
      "\n",
      "                                                      mse_stdev     mae_avg  \\\n",
      "results_filename                                                              \n",
      "combi_scores_elasticnet__bag_convdblfilters_lst...  0.000280291  0.00827828   \n",
      "combi_scores_elasticnet_model__tiny_bidir_20sd.csv  0.000280288  0.00888981   \n",
      "\n",
      "                                                       mae_med    mae_stdev  \\\n",
      "results_filename                                                              \n",
      "combi_scores_elasticnet__bag_convdblfilters_lst...  0.00846952  0.000453919   \n",
      "combi_scores_elasticnet_model__tiny_bidir_20sd.csv  0.00908113  0.000454009   \n",
      "\n",
      "                                                   mae_normd_dev  \n",
      "results_filename                                                  \n",
      "combi_scores_elasticnet__bag_convdblfilters_lst...      0.421319  \n",
      "combi_scores_elasticnet_model__tiny_bidir_20sd.csv      0.421414  )\n",
      "(\"no mse main df's head\", Empty DataFrame\n",
      "Columns: [mae_avg, mae_med, mae_stdev, mape_avg, mape_med, mape_stdev, mae_normd_dev]\n",
      "Index: [])\n"
     ]
    }
   ],
   "source": [
    "#create MAIN dataframe (all models and all results) for those without MSE\n",
    "no_mse_main_colnames =['results_filename','mae_avg','mae_med','mae_stdev','mape_avg','mape_med','mape_stdev','mae_normd_dev']\n",
    "no_mse_main_df = pd.DataFrame(columns=no_mse_main_colnames)\n",
    "no_mse_main_df.set_index(keys=['results_filename'],inplace=True)\n",
    "\n",
    "#create MAIN dataframe with MSE \n",
    "mse_main_colnames = ['results_filename','mse_avg','mse_med','mse_stdev','mae_avg','mae_med','mae_stdev','mae_normd_dev']\n",
    "mse_main_df = pd.DataFrame(columns=mse_main_colnames)\n",
    "mse_main_df.set_index(keys=['results_filename'], inplace=True)\n",
    "#colnames: mape, mse,mae {mean, med, stdev}, metric 1 \n",
    "\n",
    "#just do it a second time becausethe clutter is too much\n",
    "for results_file in results_files_list:\n",
    "    interm_df = pd.read_csv(results_aggregation_path + results_file)\n",
    "    print(\"results file: \", str(results_file))\n",
    "    #check if it has mse or not\n",
    "    if 'mse' not in interm_df.columns: #aggregate into the no_mse_main\n",
    "        keys_to_aggregate = ['mae','mape']\n",
    "        for key in keys_to_aggregate:\n",
    "            key_avg_colname = str(key) + \"_\" + \"avg\"\n",
    "            key_med_colname = str(key) + \"_\" + \"med\"\n",
    "            key_std_colname = str(key) + \"_\" + \"stdev\"\n",
    "#             no_mse_main_df.loc[str(results_file),[key_avg_colname]] = interm_df.loc[:,[str(key)]].mean(axis=1).values\n",
    "#             no_mse_main_df.loc[str(results_file),[key_med_colname]] = interm_df.loc[:,[str(key)]].median(axis=1).values\n",
    "#             no_mse_main_df.loc[str(results_file),[key_std_colname]] = interm_df.loc[:,[str(key)]].std(axis=1).values\n",
    "            no_mse_main_df.loc[str(results_file),[key_avg_colname]] = interm_df[str(key)].mean()\n",
    "            no_mse_main_df.loc[str(results_file),[key_med_colname]] = interm_df[str(key)].median()\n",
    "            no_mse_main_df.loc[str(results_file),[key_std_colname]] = interm_df[str(key)].std()\n",
    "            \n",
    "    if 'mse' in interm_df.columns: #aggregate into the mse_main\n",
    "        keys_to_aggregate = ['mse','mae'] #I took mape out. \n",
    "        for key in keys_to_aggregate:\n",
    "            key_avg_colname = str(key) + \"_\" + \"avg\"\n",
    "            key_med_colname = str(key) + \"_\" + \"med\"\n",
    "            key_std_colname = str(key) + \"_\" + \"stdev\"\n",
    "#             mse_main_df.loc[str(results_file),[key_avg_colname]] = interm_df.loc[:,[str(key)]].mean(axis=1).values\n",
    "#             mse_main_df.loc[str(results_file),[key_med_colname]] = interm_df.loc[:,[str(key)]].median(axis=1).values\n",
    "#             mse_main_df.loc[str(results_file),[key_std_colname]] = interm_df.loc[:,[str(key)]].std(axis=1).values\n",
    "            mse_main_df.loc[str(results_file),[key_avg_colname]] = interm_df[str(key)].mean()\n",
    "            mse_main_df.loc[str(results_file),[key_med_colname]] = interm_df[str(key)].median()\n",
    "            mse_main_df.loc[str(results_file),[key_std_colname]] = interm_df[str(key)].std()\n",
    "\n",
    "            \n",
    "#mse_main_df.set_index(keys=['results_filename'],inplace=True) #inplace True deletes the column I used as index..\n",
    "mse_main_df.sort_values(by=['mae_avg'],inplace=True)\n",
    "mse_main_df.loc[:,['mae_normd_dev']] = (mse_main_df.loc[:,['mae_med']].values - mse_main_df.loc[:,['mae_avg']].values)/ (mse_main_df.loc[:,['mae_stdev']].values)\n",
    "print(\"mse main df's head\", mse_main_df.head(2))\n",
    "\n",
    "#no_mse_main_df.set_index(keys=['results_filename'],inplace=True)\n",
    "no_mse_main_df.sort_values(by=['mae_avg'],inplace=True)\n",
    "no_mse_main_df.loc[:,['mae_normd_dev']] = (no_mse_main_df.loc[:,['mae_med']].values - no_mse_main_df.loc[:,['mae_avg']].values)/ (no_mse_main_df.loc[:,['mae_stdev']].values)\n",
    "\n",
    "#no_mse_main_df.reset_index\n",
    "print(\"no mse main df's head\", no_mse_main_df.head(2))\n",
    "\n",
    "no_mse_main_df.to_csv(\"./analysis/no_mse_main_df.csv\")\n",
    "mse_main_df.to_csv(\"./analysis/mse_main_df.csv\")\n",
    "        #mean(axis=1)\n",
    "        #median(axis=1)\n",
    "        #std(axis=1)\n",
    "        #aggregate and stuff into the no_mse_main_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO: pca whiten=True.\n",
    "##################################################### OLD STUFF BELOW\n",
    "# #load into generator\n",
    "#\n",
    "#\n",
    "# # instantiate variables\n",
    "# mse_cumulative = 0.0\n",
    "# mse_at_instance = 0.0\n",
    "# mse_average = 0.0\n",
    "# mse_at_instance_list = []\n",
    "# mse_average_list = []\n",
    "# mse_cumulative_list = []\n",
    "#\n",
    "# mae_cumulative = 0.0\n",
    "# mae_at_instance = 0.0\n",
    "# mae_average = 0.0\n",
    "# mae_at_instance_list = []\n",
    "# mae_average_list = []\n",
    "# mae_cumulative_list = []\n",
    "#\n",
    "# # med_ae_cumulative = 0.0\n",
    "# # med_ae_at_instance = 0.0\n",
    "# # med_ae_average = 0.0\n",
    "# # med_ae_at_instance_list = []\n",
    "# # med_ae_average_list = []\n",
    "# # med_ae_cumulative_list = []\n",
    "#\n",
    "# msle_cumulative = 0.0\n",
    "# msle_at_instance = 0.0\n",
    "# msle_average = 0.0\n",
    "# msle_at_instance_list = []\n",
    "# msle_average_list = []\n",
    "# msle_cumulative_list = []\n",
    "#\n",
    "# evs_cumulative = 0.0\n",
    "# evs_at_instance = 0.0\n",
    "# evs_average = 0.0\n",
    "# evs_at_instance_list = []\n",
    "# evs_average_list = []\n",
    "# evs_cumulative_list = []\n",
    "#\n",
    "# r2_cumulative = 0.0\n",
    "# r2_at_instance = 0.0\n",
    "# r2_average = 0.0\n",
    "# r2_at_instance_list = []\n",
    "# r2_average_list = []\n",
    "# r2_cumulative_list = []\n",
    "#\n",
    "# # for index_to_load in range(0,2):\n",
    "# for index_to_load in range(0, len(test_seqs_filenames)):\n",
    "#\n",
    "#     mse_cumulative = 0.0\n",
    "#     mse_at_instance = 0.0\n",
    "#     mse_average = 0.0\n",
    "#     mse_at_instance_list = []\n",
    "#     mse_average_list = []\n",
    "#     mse_cumulative_list = []\n",
    "#\n",
    "#     mae_cumulative = 0.0\n",
    "#     mae_at_instance = 0.0\n",
    "#     mae_average = 0.0\n",
    "#     mae_at_instance_list = []\n",
    "#     mae_average_list = []\n",
    "#     mae_cumulative_list = []\n",
    "#\n",
    "#     # med_ae_cumulative = 0.0\n",
    "#     # med_ae_at_instance = 0.0\n",
    "#     # med_ae_average = 0.0\n",
    "#     # med_ae_at_instance_list = []\n",
    "#     # med_ae_average_list = []\n",
    "#     # med_ae_cumulative_list = []\n",
    "#\n",
    "#     # msle_cumulative = 0.0\n",
    "#     # msle_at_instance = 0.0\n",
    "#     # msle_average = 0.0\n",
    "#     # msle_at_instance_list = []\n",
    "#     # msle_average_list = []\n",
    "#     # msle_cumulative_list = []\n",
    "#\n",
    "#     evs_cumulative = 0.0\n",
    "#     evs_at_instance = 0.0\n",
    "#     evs_average = 0.0\n",
    "#     evs_at_instance_list = []\n",
    "#     evs_average_list = []\n",
    "#     evs_cumulative_list = []\n",
    "#\n",
    "#     r2_cumulative = 0.0\n",
    "#     r2_at_instance = 0.0\n",
    "#     r2_average = 0.0\n",
    "#     r2_at_instance_list = []\n",
    "#     r2_average_list = []\n",
    "#     r2_cumulative_list = []\n",
    "#\n",
    "#     files = combined_filenames[index_to_load]\n",
    "#     print(\"files: {}\".format(files))\n",
    "#     preds_load_path = preds_path + files[0]\n",
    "#     test_label_load_path = test_labels_path + files[1]\n",
    "#     preds_array_temp = np.load(preds_load_path)\n",
    "#     label_test_array = np.load(test_label_load_path)\n",
    "#     print(\"before changing. preds shape: {}, label shape: {}\".format(preds_array_temp.shape, label_test_array.shape))\n",
    "#     if label_test_array.shape[1] > 5:\n",
    "#         label_test_array = label_test_array[:, 1:]\n",
    "#\n",
    "#     # TODO: reshape the preds.\n",
    "#     preds_array = np.reshape(preds_array_temp, newshape=(preds_array_temp.shape[1], 4))\n",
    "#     identifier = files[1][:-4]\n",
    "#     mse_full = mean_squared_error(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0], 1:])\n",
    "#     # mse_full_vw = mean_squared_error(y_pred=preds_array,y_true=label_test_array[0:preds_array.shape[0],1:],multioutput='variance_weighted')\n",
    "#     mae_full = mean_absolute_error(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0], 1:])\n",
    "#     # mae_full_vw = mean_absolute_error(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0],1:],multioutput='variance_weighted')\n",
    "#     r2_full = r2_score(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0], 1:])\n",
    "#     # r2_full_vw = r2_score(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0],1:],multioutput='variance_weighted')\n",
    "#     evs_full = explained_variance_score(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0], 1:])\n",
    "#     # evs_full_vw = explained_variance_score(y_pred=preds_array, y_true=label_test_array[0:preds_array.shape[0],1:],multioutput='variance_weighted')\n",
    "#\n",
    "#     # if train_array.shape[1] > 11:\n",
    "#     #     train_array = train_array[:,1:]\n",
    "#\n",
    "#     # identifier = files[0][:-4]\n",
    "#\n",
    "#     # TODO load predictions\n",
    "#     # TODO load labels\n",
    "#     # initialize two sklearn metrics\n",
    "#\n",
    "#     # loss_cumulative = loss_temp + mean_squared_error(y_true=label_train_array,y_pred=train_array[:,-4:])\n",
    "#     # loss_at_instance = mean_squared_error(y_true=None,y_pred=None)\n",
    "#     # loss_average = loss_cumulative / 5#counter #BASED ON LOSS CUMULATIVE\n",
    "#     # loss_instance_avg = ?\n",
    "#\n",
    "#     chunker_proto_preds = generator_chunker(array_raw=preds_array, chunker_batch_size=CHUNKER_BATCH_TRAVERSAL_SIZE,\n",
    "#                                             start_at=0,\n",
    "#                                             scaler_active=False)\n",
    "#     chunker_proto_label = generator_chunker(array_raw=label_test_array, chunker_batch_size=CHUNKER_BATCH_TRAVERSAL_SIZE,\n",
    "#                                             start_at=0,\n",
    "#                                             scaler_active=True, scaler_type='standard_per_batch')\n",
    "#\n",
    "#     remaining = CHUNKER_BATCH_SIZE * (preds_array.shape[0] // CHUNKER_BATCH_SIZE)\n",
    "#     counter = 0\n",
    "#     # TODO modify this. load both generators and just accumulate the loss.\n",
    "#     while remaining > 0:\n",
    "#         counter = counter + 1\n",
    "#         chunk_preds = chunker_proto_preds.next()\n",
    "#         # chunk_data = chunk_data[:,-4:] #dummy, just cut the array to the last 4 columns.\n",
    "#         chunk_label = chunker_proto_label.next()\n",
    "#         chunk_label = chunk_label[:, 1:]\n",
    "#\n",
    "#         # MSE\n",
    "#         mse_at_instance = mean_squared_error(y_true=chunk_label, y_pred=chunk_preds)\n",
    "#         mse_at_instance_list.append(mse_at_instance)\n",
    "#         mse_cumulative = mse_cumulative + mse_at_instance\n",
    "#         mse_cumulative_list.append(mse_cumulative)\n",
    "#         mse_average = mse_cumulative / counter\n",
    "#         mse_average_list.append(mse_average)\n",
    "#\n",
    "#         # MAE\n",
    "#         mae_at_instance = mean_absolute_error(y_true=chunk_label, y_pred=chunk_preds)\n",
    "#         mae_at_instance_list.append(mae_at_instance)\n",
    "#         mae_cumulative = mae_cumulative + mae_at_instance\n",
    "#         mae_cumulative_list.append(mae_cumulative)\n",
    "#         mae_average = mae_cumulative / counter\n",
    "#         mae_average_list.append(mae_average)\n",
    "#\n",
    "#         # MSLE\n",
    "#         # msle_at_instance = mean_squared_log_error(y_true=chunk_label,y_pred=chunk_data)\n",
    "#         # msle_at_instance_list.append(msle_at_instance)\n",
    "#         # msle_cumulative = msle_cumulative + msle_at_instance\n",
    "#         # msle_cumulative_list.append(msle_cumulative)\n",
    "#         # msle_average = msle_cumulative/counter\n",
    "#         # msle_average_list.append(msle_average)\n",
    "#\n",
    "#         # R2\n",
    "#         r2_at_instance = r2_score(y_true=chunk_label, y_pred=chunk_preds)\n",
    "#         r2_at_instance_list.append(r2_at_instance)\n",
    "#         r2_cumulative = r2_cumulative + r2_at_instance\n",
    "#         r2_cumulative_list.append(r2_cumulative)\n",
    "#         r2_average = r2_cumulative / counter\n",
    "#         r2_average_list.append(r2_average)\n",
    "#\n",
    "#         # EVS\n",
    "#         evs_at_instance = explained_variance_score(y_true=chunk_label, y_pred=chunk_preds)\n",
    "#         evs_at_instance_list.append(evs_at_instance)\n",
    "#         evs_cumulative = evs_cumulative + evs_at_instance\n",
    "#         evs_cumulative_list.append(evs_cumulative)\n",
    "#         evs_average = evs_cumulative / counter\n",
    "#         evs_average_list.append(evs_average)\n",
    "#\n",
    "#         # Med_AE can't do multiple columns at once!\n",
    "#         # med_ae_at_instance = median_absolute_error(y_true=chunk_label,y_pred=chunk_data) #\n",
    "#         # med_ae_at_instance_list.append(med_ae_at_instance)\n",
    "#         # med_ae_cumulative = med_ae_cumulative + med_ae_at_instance\n",
    "#         # med_ae_cumulative_list.append(med_ae_cumulative)\n",
    "#         # med_ae_average = med_ae_cumulative/counter\n",
    "#         # med_ae_average_list.append(med_ae_average)\n",
    "#\n",
    "#         print(\"remaining: {}\".format(remaining))\n",
    "#         remaining = remaining - CHUNKER_BATCH_SIZE\n",
    "#         # print(\"data chunk 2: {}\".format(chunker_proto_data.next()))\n",
    "#\n",
    "#     aggregate_list = []  # saves the aggregated array in a list so the filename saving can be automated\n",
    "#     aggregate_name_list = []  # since trying to directly access variable names isn't a good idea in python...\n",
    "#     # MSE MAE MSLE R2 EVS MED_AE\n",
    "#     print(\"mse_cumulative: {}\".format(mse_cumulative_list))\n",
    "#     print(\"mse_average: {}\".format(mse_average_list))\n",
    "#     print(\"mse_at_instance: {}\".format(mse_at_instance_list))\n",
    "#     assert (len(mse_cumulative_list) == len(mse_average_list) == len(mse_at_instance_list))\n",
    "#     aggregate_mse = np.empty(shape=(len(mse_cumulative_list), 4))\n",
    "#     # ORDER IS cumulative - average - at instance\n",
    "#     aggregate_mse[:, 0] = np.asarray(mse_cumulative_list)\n",
    "#     aggregate_mse[:, 1] = np.asarray(mse_average_list)\n",
    "#     aggregate_mse[:, 2] = np.asarray(mse_at_instance_list)\n",
    "#     aggregate_mse[0, 3] = mse_full\n",
    "#     # aggregate_mse[1, 3] = mse_full_vw\n",
    "#     aggregate_list.append(aggregate_mse)\n",
    "#     aggregate_name_list.append('mse_agg')\n",
    "#\n",
    "#     print(\"mae_cumulative: {}\".format(mae_cumulative_list))\n",
    "#     print(\"mae_average: {}\".format(mae_average_list))\n",
    "#     print(\"mae_at_instance: {}\".format(mae_at_instance_list))\n",
    "#     assert (len(mae_cumulative_list) == len(mae_average_list) == len(mae_at_instance_list))\n",
    "#     aggregate_mae = np.empty(shape=(len(mae_cumulative_list), 4))\n",
    "#     # ORDER IS cumulative - average - at instance\n",
    "#     aggregate_mae[:, 0] = np.asarray(mae_cumulative_list)\n",
    "#     aggregate_mae[:, 1] = np.asarray(mae_average_list)\n",
    "#     aggregate_mae[:, 2] = np.asarray(mae_at_instance_list)\n",
    "#     aggregate_mae[0, 3] = mae_full\n",
    "#     aggregate_mse[1:, 3] = 0.0\n",
    "#     aggregate_list.append(aggregate_mae)\n",
    "#     aggregate_name_list.append('mae_agg')\n",
    "#\n",
    "#     # print(\"msle_cumulative: {}\".format(msle_cumulative_list))\n",
    "#     # print(\"msle_average: {}\".format(msle_average_list))\n",
    "#     # print(\"msle_at_instance: {}\".format(msle_at_instance_list))\n",
    "#     # assert(len(msle_cumulative_list)==len(msle_average_list)==len(msle_at_instance_list))\n",
    "#     # aggregate_msle = np.empty(shape=(len(msle_cumulative_list),4))\n",
    "#     # #ORDER IS cumulative - average - at instance\n",
    "#     # aggregate_msle[:, 0] = np.asarray(msle_cumulative_list)\n",
    "#     # aggregate_msle[:, 1] = np.asarray(msle_average_list)\n",
    "#     # aggregate_msle[:, 2] = np.asarray(msle_at_instance_list)\n",
    "#     # aggregate_msle[0, 3] = msle_full\n",
    "#     # aggregate_list.append(aggregate_msle)\n",
    "#     # aggregate_name_list.append(\"msle\")\n",
    "#\n",
    "#     print(\"r2_cumulative: {}\".format(r2_cumulative_list))\n",
    "#     print(\"r2_average: {}\".format(r2_average_list))\n",
    "#     print(\"r2_at_instance: {}\".format(r2_at_instance_list))\n",
    "#     assert (len(r2_cumulative_list) == len(r2_average_list) == len(r2_at_instance_list))\n",
    "#     aggregate_r2 = np.empty(shape=(len(r2_cumulative_list), 4))\n",
    "#     # ORDER IS cumulative - average - at instance\n",
    "#     aggregate_r2[:, 0] = np.asarray(r2_cumulative_list)\n",
    "#     aggregate_r2[:, 1] = np.asarray(r2_average_list)\n",
    "#     aggregate_r2[:, 2] = np.asarray(r2_at_instance_list)\n",
    "#     aggregate_r2[0, 3] = r2_full\n",
    "#     aggregate_list.append(aggregate_r2)\n",
    "#     aggregate_name_list.append('r2_agg')\n",
    "#\n",
    "#     print(\"evs_cumulative: {}\".format(evs_cumulative_list))\n",
    "#     print(\"evs_average: {}\".format(evs_average_list))\n",
    "#     print(\"evs_at_instance: {}\".format(evs_at_instance_list))\n",
    "#     assert (len(evs_cumulative_list) == len(evs_average_list) == len(evs_at_instance_list))\n",
    "#     aggregate_evs = np.empty(shape=(len(evs_cumulative_list), 4))\n",
    "#     # ORDER IS cumulative - average - at instance\n",
    "#     aggregate_evs[:, 0] = np.asarray(evs_cumulative_list)  # cumulative.\n",
    "#     aggregate_evs[:, 1] = np.asarray(evs_average_list)\n",
    "#     aggregate_evs[:, 2] = np.asarray(evs_at_instance_list)\n",
    "#     aggregate_evs[0, 3] = evs_full\n",
    "#     aggregate_list.append(aggregate_evs)\n",
    "#     aggregate_name_list.append('evs_agg')\n",
    "#\n",
    "#     if save_arrays == True:\n",
    "#         for index in range(0, len(aggregate_list)):\n",
    "#             # get the index of the array in the list of names (the second list)\n",
    "#             arrayname = aggregate_name_list[index]\n",
    "#             np.savetxt(fname=chunker_path + arrayname + \"_\" + str(identifier) + \".csv\", delimiter=',',\n",
    "#                        X=aggregate_list[index], header=\"cumulative(sum)-average-instance\", fmt='%.18e')\n",
    "#\n",
    "#\n",
    "#             # TODO: aggregate and save as a numpy array or a csv.\n",
    "#\n",
    "#             # print(\"med_ae_cumulative: {}\".format(med_ae_cumulative_list))\n",
    "#             # print(\"med_ae_average: {}\".format(med_ae_average_list))\n",
    "#             # print(\"med_ae_at_instance: {}\".format(med_ae_at_instance_list))\n",
    "#             # print(\"label chunk 1: {}\".format(chunker_proto_label.next()))\n",
    "#             # print(\"label chunk 2: {}\".format(chunker_proto_label.next()))\n",
    "#\n",
    "#             # if (str(files[0]) == 'sequence_2c_288_9_fv1b.npy') == True:\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:, 0], '^', label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:, 0], '.', label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.75 * (len(y_prediction)), 1 * (len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction from 75% - 100% of the sequence on Crack 01')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_0_conv_75_100_newmarker_batch' + str(\n",
    "#             #         generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:, 1], '^', label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:, 1], 'v', label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.75 * (len(y_prediction)), 1 * (len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 75% - 100% of the sequence on Crack 02')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_1_conv_75_100_newmarker_batch' + str(\n",
    "#             #         generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:, 2], '^', label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:, 2], 'v', label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.75 * (len(y_prediction)), 1 * (len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 75% - 100% of the sequence on Crack 03')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_2_conv_75_100_newmarker_batch' + str(\n",
    "#             #         generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:, 3], '^', label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:, 3], 'v', label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.75 * (len(y_prediction)), 1 * (len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 75% - 100% of the sequence on Crack 04')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_3_conv_75_100_newmarker_batch' + str(\n",
    "#             #         generator_batch_size) + '_.png')\n",
    "#             # DEVIN PLOT CODE\n",
    "#             # if save_figs == True:\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:,0],'^',label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:,0],'.',label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.5*(len(y_prediction)), 1*(len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction from 50% - 100% of the sequence on Crack 01')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_0_conv_50_100_newmarker_batch' + str(generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:,1],'^',label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:,1],'v',label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.5*(len(y_prediction)), 1*(len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 50% - 100% of the sequence on Crack 02')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_1_conv_50_100_newmarker_batch' + str(generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:,2],'^',label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:,2],'v',label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.5*(len(y_prediction)), 1*(len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 50% - 100% of the sequence on Crack 03')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_2_conv_50_100_newmarker_batch' + str(generator_batch_size) + '_.png')\n",
    "#             #\n",
    "#             #     plt.clf()\n",
    "#             #     plt.cla()\n",
    "#             #     plt.close()\n",
    "#             #     plt.plot(label_truth[:,3],'^',label=\"ground truth\", markersize=5)\n",
    "#             #     plt.plot(y_prediction[:,3],'v',label=\"prediction\", markersize=4)\n",
    "#             #     plt.xscale('log')\n",
    "#             #     plt.xlabel('# Cycle(s)')\n",
    "#             #     plt.yscale('log')\n",
    "#             #     plt.ylabel('Value(s)')\n",
    "#             #     plt.legend()\n",
    "#             #     plt.xlim((0.5*(len(y_prediction)), 1*(len(y_prediction))))\n",
    "#             #     plt.title('truth vs prediction  from 50% - 100% of the sequence on Crack 04')\n",
    "#             #     plt.grid(True)\n",
    "#             #     plt.savefig('results_' + str(files[0]) + '_flaw_3_conv_50_100_newmarker_batch' + str(generator_batch_size) + '_.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
